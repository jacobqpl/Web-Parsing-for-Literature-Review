{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the earliest date in m/d/yyyy format:  12/10/2000\n",
      "Enter the latest date in m/d/yyyy format:  12/30/2019\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import requests\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "all_pmids = [] #This is the list where we will put all the PMIDs, loop them, and make them into URLs. \n",
    "out = []\n",
    "\n",
    "#enter the start date range here - this will eventually go in the URL \n",
    "startdate = input('Enter the earliest date in m/d/yyyy format: ')\n",
    "\n",
    "sm = startdate.split('/')[0]\n",
    "sd = startdate.split('/')[1]\n",
    "sy = startdate.split('/')[2]\n",
    "\n",
    "sm = int(sm)\n",
    "sd = int(sd)\n",
    "sy = int(sy)\n",
    "\n",
    "sm2 = startdate.split('/')[0]\n",
    "sd2 = startdate.split('/')[1]\n",
    "sy2 = startdate.split('/')[2]\n",
    "\n",
    "sm2 = str(sm2)\n",
    "sd2 = str(sd2)\n",
    "sy2 = str(sy2)\n",
    "\n",
    "if sm < 10 and '0' not in sm2:\n",
    "    sm2 = '0' + sm2\n",
    "\n",
    "if sd < 10 and '0' not in sd2:\n",
    "    sd2 = '0' + sd2\n",
    "\n",
    "#enter the end date range here this will eventually go in the URL \n",
    "enddate = input('Enter the latest date in m/d/yyyy format: ')\n",
    "\n",
    "m = enddate.split('/')[0]\n",
    "d = enddate.split('/')[1]\n",
    "y = enddate.split('/')[2]\n",
    "\n",
    "m = int(m)\n",
    "d = int(d)\n",
    "y = int(y)\n",
    "\n",
    "m2 = enddate.split('/')[0]\n",
    "d2 = enddate.split('/')[1]\n",
    "y2 = enddate.split('/')[2]\n",
    "\n",
    "m2 = str(m2)\n",
    "d2 = str(d2)\n",
    "y2 = str(y2)\n",
    "\n",
    "\n",
    "if m < 10 and '0' not in m2:\n",
    "    m2 = '0' + m2\n",
    "\n",
    "if d < 10 and '0' not in d2:\n",
    "    d2 = '0' + d2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for: telehealth+access\n",
      "Searching for: access+to+care\n"
     ]
    }
   ],
   "source": [
    "# Below is the text file with your terms.  If you have more than one word in a search term, you can either add a plus in the list itself or use the split command\n",
    "# You want it to look like this: this+is+my+search+term\n",
    "terms= open(\"lit_search_terms.txt\", \"r\") \n",
    "terms = terms.read().split('\\n')\n",
    "\n",
    "for term in terms: #at the end of the url in full_url is a number.  This will indicate the number of results that can show up in a search.  The max number is 200. Right now, I have it at 20. \n",
    "    full_url = 'https://pubmed.ncbi.nlm.nih.gov/?term=%28'+term+'%5BText+Word%5D%29+AND+%28%28%22'+ sy2 +'%2F'+ sm2 + '%2F'+ sd2 +'%22%5BDate+-+Publication%5D+%3A+%22'+y2+'%2F'+m2+'%2F'+d2+'%22%5BDate+-+Publication%5D%29%29&sort=pubdate&size=20'                                                  \n",
    "    response = requests.get(full_url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    print('Searching for: '+term)\n",
    "    #When there's just one result, it automatically takes you to the one abstract the shows up.  Unfortunately this messes with the process of extracting all PMIDs and throwing them into a list.\n",
    "    #When this happens, you'd just tell it to start grabbing the abstract information if the result is 1.  If it's more than 1, start grabbing the PMIDs per usual. \n",
    "    results = '1' if soup.find('span', {'class' : 'single-result-redirect-message'}) is not None else '>1'\n",
    " \n",
    "\n",
    "    if results == '1': #Basically, we are saying that if there are no results, just look at the 1, get the PMID and throw it into list with other searches. \n",
    "        singlepmid = soup.find(\"meta\",  property=\"og:url\")\n",
    "        singlepmid = str(singlepmid)\n",
    "        singlepmid = singlepmid.split('<meta content=\"https://pubmed.ncbi.nlm.nih.gov/')[1]\n",
    "        singlepmid = singlepmid.split('/')[0]\n",
    "        all_pmids.append(singlepmid)\n",
    "    else:\n",
    "        singlepmid = None\n",
    "\n",
    "    if results == '>1': #Most searches will have more than 1 result. In this case, it gets the PMIDs/URLs for each abstract and throws it into a list.\n",
    "           \n",
    "        pmids = soup.find_all('span', {'class' : 'docsum-pmid'})\n",
    "        for p in pmids:\n",
    "            p = p.get_text()\n",
    "            all_pmids.append(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "    for pmid in all_pmids: #looping through the list of PMIDs\n",
    "        url = 'https://pubmed.ncbi.nlm.nih.gov/'+pmid #each abstract URL is literally the same baseline URL with / and the PMID after\n",
    "        response2 = requests.get(url)\n",
    "        soup2 = BeautifulSoup(response2.content, 'html.parser')\n",
    "\n",
    "        #Below we are getting the authors, journal, link to the full journal article, the abstract, and author affiliation\n",
    "        #There are cleaner ways to do this, but for me it's faster to just find the data and parse the text around it.\n",
    "        #If something is straightforward like a title or a table, you usually can just do soup.find('thehtmltag') and then .get_text() to get rid of the HTML tags\n",
    "\n",
    "        #Authors\n",
    "        authors = soup2.find(class_='authors-list').get_text(' ') \n",
    "        authors = authors.rstrip()\n",
    "        authors = \" \".join(authors.split())\n",
    "        authors = authors.replace(' , ', ', ')\n",
    "\n",
    "        #Journal\n",
    "        journal = soup2.find(class_='journal-actions-trigger trigger') if soup2.find(class_='journal-actions-trigger trigger') is not None else ''\n",
    "        journal = str(journal) if soup2.find(class_='journal-actions-trigger trigger') is not None else ''\n",
    "        journal = journal.split('title=\"')[1] if soup2.find(class_='journal-actions-trigger trigger') is not None else ''\n",
    "        journal = journal.split('\">')[0] if soup2.find(class_='journal-actions-trigger trigger') is not None else ''\n",
    "        journal = journal.title() if soup2.find(class_='journal-actions-trigger trigger') is not None else ''\n",
    "\n",
    "        #Link to full article\n",
    "\n",
    "        fulltext = soup2.find(class_='full-text-links-list') if soup2.find(class_='full-text-links-list') is not None else ''\n",
    "        fulltext = str(fulltext) if soup2.find(class_='full-text-links-list') is not None else ''\n",
    "        fulltext = fulltext.split('href=\"')[1] if soup2.find(class_='full-text-links-list') is not None else ''\n",
    "        fulltext = fulltext.split('\"')[0] if soup2.find(class_='full-text-links-list') is not None else ''\n",
    "\n",
    "        # Abstract\n",
    "        abstract = soup2.find(class_='abstract-content selected')\n",
    "        abstract = str(abstract)\n",
    "        abstract = abstract.rstrip()\n",
    "        abstract = abstract.replace('<p>', '@p@')\n",
    "        clean = re.compile('<.*?>')\n",
    "        abstract = re.sub(clean, '', abstract)\n",
    "        abstract = re.sub(r'\\n\\s*\\n', '\\n\\n', abstract)\n",
    "        abstract = \" \".join(abstract.split())\n",
    "        abstract = abstract.replace('@p@ ','\\n'+'\\n')\n",
    "        abstract = abstract.lstrip()\n",
    "        abstract = abstract.rstrip()\n",
    "\n",
    "        # Affiliation\n",
    "        affiliation = soup2.find(class_='item-list').get_text(' ') if soup2.find(class_='item-list') is not None else ''\n",
    "        affiliation = str(affiliation)if soup2.find(class_='item-list')is not None else ''\n",
    "        affiliation = affiliation.split('\\n') if soup2.find(class_='item-list') is not None else ''\n",
    "        affiliation = str(affiliation)  if soup2.find(class_='item-list') is not None else ''\n",
    "        affiliation = affiliation.replace(\" ', '']\",\"\").replace(\"['', ' \",\"\").replace(\"', '\",\",\") if soup2.find(class_='item-list')is not None else ''\n",
    "    \n",
    "        title = soup2.select('h1.heading-title')[0].text.strip()  \n",
    "\n",
    "        data = {'title': title, 'authors': authors, 'affiliations': affiliation, 'abstract': abstract, 'journal': journal,'full_article_link': fulltext, 'url':url}\n",
    "        time.sleep(5) #the robots.txt file on PubMed states that they want 5 seconds between requests. Change this as needed. \n",
    "\n",
    "        out.append(data) #We're throwing all the data into a new list and creating a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(out) #This is the dataframe\n",
    "df.drop_duplicates() # This drops any duplicates.\n",
    "df.to_excel('HPV.xlsx') #See the Pandas module for this.  If you wanted this to be a csv file, you'd just change it to df.to_csv('nameofyouroutput.csv). \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
